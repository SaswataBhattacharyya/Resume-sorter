{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from docx import Document\n",
    "from lxml import etree\n",
    "import win32com.client  # For handling .doc files\n",
    "from PyPDF2 import PdfReader  # For handling .pdf files\n",
    "import ollama\n",
    "\n",
    "def extract_text_from_shapes_and_textboxes(doc):\n",
    "    extracted_text = []\n",
    "    for shape in doc.element.xpath('.//w:txbxContent//w:t'):\n",
    "        extracted_text.append(shape.text)\n",
    "    return ' '.join(extracted_text).strip()\n",
    "\n",
    "def extract_text_from_headers_and_footers(doc):\n",
    "    extracted_text = []\n",
    "    for section in doc.sections:\n",
    "        for paragraph in section.header.paragraphs:\n",
    "            extracted_text.append(paragraph.text)\n",
    "        for paragraph in section.footer.paragraphs:\n",
    "            extracted_text.append(paragraph.text)\n",
    "    return ' '.join(extracted_text).strip()\n",
    "\n",
    "def doc_to_txt(input_path, output_path):\n",
    "    try:\n",
    "        word = win32com.client.Dispatch(\"Word.Application\")\n",
    "        doc = word.Documents.Open(input_path)\n",
    "        doc_text = doc.Content.Text\n",
    "        doc.Close()\n",
    "        word.Quit()\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "            txt_file.write(doc_text)\n",
    "        print(f\"Converted: {input_path} -> {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {input_path}: {e}\")\n",
    "\n",
    "def pdf_to_txt(input_path, output_path):\n",
    "    try:\n",
    "        reader = PdfReader(input_path)\n",
    "        extracted_text = []\n",
    "        for page in reader.pages:\n",
    "            extracted_text.append(page.extract_text())\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "            txt_file.write('\\n'.join(extracted_text))\n",
    "        print(f\"Converted: {input_path} -> {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {input_path}: {e}\")\n",
    "\n",
    "def split_large_text(content, max_chars=7000):\n",
    "    chunks = []\n",
    "    while len(content) > max_chars:\n",
    "        split_index = content[:max_chars].rfind(\" \")\n",
    "        chunks.append(content[:split_index])\n",
    "        content = content[split_index:].strip()\n",
    "    chunks.append(content)\n",
    "    return chunks\n",
    "\n",
    "def summarize_with_ollama(chunks, file_name):\n",
    "    combined_summary = []\n",
    "    name = None\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        try:\n",
    "            print(f\"Summarizing chunk {i + 1} of {len(chunks)}...\")\n",
    "            prompt = (\n",
    "                f\"Summarize the following text while retaining key details relevant to the resume format.\\n\"\n",
    "                f\"Focus on professional experience, education, skills, and other resume-relevant content.\\n\"\n",
    "                f\"Rules:\\n\"\n",
    "                f\"- Name: Refer to the file name ({file_name}) if not explicitly mentioned in the text.\\n\"\n",
    "                f\"- Age in years\\n- Qualification\\n\"\n",
    "                f\"- Subject Area of Highest Qualification\\n- Place of Education for Highest Qualification\\n\"\n",
    "                f\"- Coding language\\n- Spoken language\\n- Skill set\\n\"\n",
    "                f\"- Years of work experience\\n- Any links given/email-ID.\\n\"\n",
    "                f\"Note: This text is part {i + 1} of {len(chunks)} from a larger document. Ensure continuity while summarizing.\\n\"\n",
    "                f\"Here is the text: {chunk}\"\n",
    "            )\n",
    "            response = ollama.chat(\n",
    "                model=\"llama3.2:latest\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            summary = response.get(\"message\", {}).get(\"content\", \"\")\n",
    "            if \"Name:\" in summary and not name:\n",
    "                name_line = [line for line in summary.splitlines() if line.startswith(\"Name:\")]\n",
    "                if name_line:\n",
    "                    name = name_line[0]\n",
    "            combined_summary.append(summary)\n",
    "        except Exception as e:\n",
    "            print(f\"Error summarizing chunk {i + 1}: {e}\")\n",
    "\n",
    "    # Ensure name is retained if found in any chunk\n",
    "    if name:\n",
    "        for i in range(len(combined_summary)):\n",
    "            if \"Name:\" not in combined_summary[i]:\n",
    "                combined_summary[i] = name + \"\\n\" + combined_summary[i]\n",
    "\n",
    "    return \"\\n\\n\".join(combined_summary)\n",
    "\n",
    "def convert_files_to_txt(input_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "\n",
    "        if filename.endswith(\".docx\"):\n",
    "            try:\n",
    "                doc = Document(input_path)\n",
    "                shapes_text = extract_text_from_shapes_and_textboxes(doc)\n",
    "                headers_footers_text = extract_text_from_headers_and_footers(doc)\n",
    "                paragraphs_text = [paragraph.text for paragraph in doc.paragraphs]\n",
    "                for table in doc.tables:\n",
    "                    for row in table.rows:\n",
    "                        row_data = [cell.text.strip() for cell in row.cells]\n",
    "                        paragraphs_text.append(' '.join(row_data))\n",
    "                combined_text = []\n",
    "                if shapes_text:\n",
    "                    combined_text.append(shapes_text)\n",
    "                if headers_footers_text:\n",
    "                    combined_text.append(headers_footers_text)\n",
    "                combined_text.extend([text for text in paragraphs_text if text.strip()])\n",
    "                full_text = '\\n'.join(combined_text)\n",
    "\n",
    "                if len(full_text) > 7000:\n",
    "                    chunks = split_large_text(full_text, max_chars=7000)\n",
    "                    summarized_text = summarize_with_ollama(chunks, filename)\n",
    "                    with open(output_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "                        txt_file.write(summarized_text)\n",
    "                else:\n",
    "                    with open(output_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "                        txt_file.write(full_text)\n",
    "\n",
    "                print(f\"Converted and summarized (if necessary): {filename} -> {output_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting {filename}: {e}\")\n",
    "\n",
    "        elif filename.endswith(\".doc\"):\n",
    "            doc_to_txt(input_path, output_path)\n",
    "\n",
    "        elif filename.endswith(\".pdf\"):\n",
    "            try:\n",
    "                reader = PdfReader(input_path)\n",
    "                extracted_text = []\n",
    "                for page in reader.pages:\n",
    "                    extracted_text.append(page.extract_text())\n",
    "                full_text = '\\n'.join(extracted_text)\n",
    "\n",
    "                if len(full_text) > 7000:\n",
    "                    chunks = split_large_text(full_text, max_chars=7000)\n",
    "                    summarized_text = summarize_with_ollama(chunks, filename)\n",
    "                    with open(output_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "                        txt_file.write(summarized_text)\n",
    "                else:\n",
    "                    with open(output_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "                        txt_file.write(full_text)\n",
    "\n",
    "                print(f\"Converted and summarized (if necessary): {filename} -> {output_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting {filename}: {e}\")\n",
    "\n",
    "input_folder = r\"C:\\Users\\polpi\\Desktop\\data science\\project\\docker_project\\resumes\"\n",
    "output_folder = r\"C:\\Users\\polpi\\Desktop\\data science\\project\\docker_project\\resume_txt\"\n",
    "convert_files_to_txt(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ollama\n",
    "\n",
    "def split_large_text(content, max_chars=7000):\n",
    "    \"\"\"Split text into chunks of at most `max_chars` characters.\"\"\"\n",
    "    chunks = []\n",
    "    while len(content) > max_chars:\n",
    "        split_index = content[:max_chars].rfind(\" \")\n",
    "        chunks.append(content[:split_index])\n",
    "        content = content[split_index:].strip()\n",
    "    chunks.append(content)\n",
    "    return chunks\n",
    "\n",
    "def summarize_with_ollama(chunks, file_name):\n",
    "    \"\"\"Summarize chunks of text using Ollama.\"\"\"\n",
    "    combined_summary = []\n",
    "    name = None\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        try:\n",
    "            print(f\"Summarizing chunk {i + 1} of {len(chunks)}...\")\n",
    "            prompt = (\n",
    "                f\"Summarize the following text while retaining key details relevant to the resume format.\\n\"\n",
    "                f\"Focus on professional experience, education, skills, and other resume-relevant content.\\n\"\n",
    "                f\"Rules:\\n\"\n",
    "                f\"- Name: Refer to the file name ({file_name}) if not explicitly mentioned in the text.\\n\"\n",
    "                f\"- Age in years\\n- Qualification\\n\"\n",
    "                f\"- Subject Area of Highest Qualification\\n- Place of Education for Highest Qualification\\n\"\n",
    "                f\"- Coding language\\n- Spoken language\\n- Skill set\\n\"\n",
    "                f\"- Years of work experience\\n- Any links given/email-ID.\\n\"\n",
    "                f\"Note: This text is part {i + 1} of {len(chunks)} from a larger document. Ensure continuity while summarizing.\\n\"\n",
    "                f\"Here is the text: {chunk}\"\n",
    "            )\n",
    "            response = ollama.chat(\n",
    "                model=\"llama3.2:latest\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            summary = response.get(\"message\", {}).get(\"content\", \"\")\n",
    "            if \"Name:\" in summary and not name:\n",
    "                name_line = [line for line in summary.splitlines() if line.startswith(\"Name:\")]\n",
    "                if name_line:\n",
    "                    name = name_line[0]\n",
    "            combined_summary.append(summary)\n",
    "        except Exception as e:\n",
    "            print(f\"Error summarizing chunk {i + 1}: {e}\")\n",
    "\n",
    "    # Ensure name is retained if found in any chunk\n",
    "    if name:\n",
    "        for i in range(len(combined_summary)):\n",
    "            if \"Name:\" not in combined_summary[i]:\n",
    "                combined_summary[i] = name + \"\\n\" + combined_summary[i]\n",
    "\n",
    "    return \"\\n\\n\".join(combined_summary)\n",
    "\n",
    "def process_txt_files(input_folder):\n",
    "    \"\"\"Scan .txt files and summarize if they exceed 7000 characters.\"\"\"\n",
    "    for filename in os.listdir(input_folder):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        if filename.endswith(\".txt\"):\n",
    "            try:\n",
    "                with open(input_path, \"r\", encoding=\"utf-8\") as txt_file:\n",
    "                    content = txt_file.read()\n",
    "\n",
    "                if len(content) > 7000:\n",
    "                    print(f\"File {filename} exceeds 7000 characters. Summarizing...\")\n",
    "                    chunks = split_large_text(content, max_chars=7000)\n",
    "                    summarized_text = summarize_with_ollama(chunks, filename)\n",
    "\n",
    "                    with open(input_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "                        txt_file.write(summarized_text)\n",
    "\n",
    "                    print(f\"File {filename} has been summarized and replaced.\")\n",
    "                else:\n",
    "                    print(f\"File {filename} is within the character limit.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Replace the path below with your folder containing .txt files\n",
    "input_folder = r\"C:\\Users\\polpi\\Desktop\\data science\\project\\docker_project\\resume_txt\"\n",
    "process_txt_files(input_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spaCy_env)",
   "language": "python",
   "name": "spacy_env"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
